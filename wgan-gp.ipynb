{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5931339,"sourceType":"datasetVersion","datasetId":1009122},{"sourceId":8999247,"sourceType":"datasetVersion","datasetId":5420954}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import necessary libraries and creating a function for visualization","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader\nfrom torchinfo import summary\nimport matplotlib.pyplot as plt\nimport glob\nfrom PIL import Image\n\n\ndef show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28)):\n    '''\n    Function for visualizing images: Given a tensor of images, number of images, and\n    size per image, plots and prints the images in an uniform grid.\n    '''\n    image_tensor = (image_tensor + 1) / 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-25T00:15:41.715903Z","iopub.execute_input":"2024-07-25T00:15:41.716612Z","iopub.status.idle":"2024-07-25T00:15:41.724305Z","shell.execute_reply.started":"2024-07-25T00:15:41.716584Z","shell.execute_reply":"2024-07-25T00:15:41.723322Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading the images into a list","metadata":{}},{"cell_type":"code","source":"images_list = []\nimages_list.append(glob.glob('/kaggle/input/siim-isic-2019-organized/dataset organized/malignant/*.jpg'))\nimages = []\nfor filepaths in images_list:\n    images.extend(filepaths)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:20:34.200643Z","iopub.execute_input":"2024-07-20T18:20:34.201020Z","iopub.status.idle":"2024-07-20T18:20:34.296640Z","shell.execute_reply.started":"2024-07-20T18:20:34.200987Z","shell.execute_reply":"2024-07-20T18:20:34.295880Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> What are WGANS? </h2>\n<p style=\"font-size: 15px;\"> A Wasserstein Generative Adversarial Network (WGAN) is an advanced type of Generative Adversarial Network (GAN) designed to improve the stability and effectiveness of training GANs. Traditional GANs often suffer from issues like mode collapse, vanishing gradients, and unstable training dynamics, which make them challenging to train. WGAN addresses these issues by using the Earth Mover's (EM) distance, also known as the Wasserstein distance, as the loss function instead of the Jensen-Shannon divergence used in standard GANs. </p>","metadata":{}},{"cell_type":"markdown","source":"<h2> The Generator </h2>\n\n<p style=\"font-size: 15px;\"> The generator in a WGAN is designed to create synthetic data samples that are indistinguishable from real data. It operates by taking in random noise vectors sampled from a latent space, which are usually drawn from a simple distribution such as a Gaussian or uniform distribution. These noise vectors are transformed through a series of neural network layers in the generator and the result of this transformation is a synthetic data sample that should closely resemble the real data the model is trying to replicate. During the training process, the generator aims to minimize the Wasserstein distance between the distribution of the generated data and the real data distribution. </p>","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim=100, im_chan=3, hidden_dim=128):\n        super(Generator, self).__init__()\n        self.z_dim = z_dim\n        # Build the neural network\n        self.gen = nn.Sequential(\n            # Z latent vector 100\n            self.make_gen_block(z_dim, hidden_dim * 8, kernel_size=4, stride=1, padding = 0), \n            # state = 1024 x 4 x 4\n            self.make_gen_block(hidden_dim * 8, hidden_dim * 4, kernel_size=4, stride=2, padding = 1),\n            # state = 512 x 8 x 8\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=2, padding = 1),\n            # state = 256 x 16 x 16\n            self.make_gen_block(hidden_dim * 2, hidden_dim, kernel_size=4, stride=2, padding = 1),\n            # state = 128 x 32 x 32\n            self.make_gen_block(hidden_dim, hidden_dim // 2, kernel_size=4, stride=2, padding = 1),\n            # state = 64 x 64 x 64\n            self.make_gen_block(hidden_dim // 2, im_chan, kernel_size=4, stride=2, padding = 1, final_layer=True),\n            # Final state = 3 x 128 x 128\n        )\n\n    def make_gen_block(self, input_channels, output_channels, kernel_size=4, stride=1, padding = 0, final_layer=False,):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding = padding),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride, padding = 1),\n                nn.Tanh(),\n            )\n\n    def forward(self, noise):\n        x = noise.view(len(noise), self.z_dim, 1, 1)\n        return self.gen(x)\n\ndef get_noise(n_samples, z_dim, device='cpu'):\n    return torch.randn(n_samples, z_dim, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:20:37.659103Z","iopub.execute_input":"2024-07-20T18:20:37.659465Z","iopub.status.idle":"2024-07-20T18:20:37.672525Z","shell.execute_reply.started":"2024-07-20T18:20:37.659424Z","shell.execute_reply":"2024-07-20T18:20:37.671580Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> The Critic </h2>\n<p style=\"font-size: 15px;\"> The critic in a WGAN replaces the discriminator used in traditional GANs. Unlike the discriminator, which classifies inputs as real or fake, the critic's job is to score the inputs in a way that approximates the Wasserstein distance between the real and generated data distributions. This scoring mechanism allows the critic to provide more informative feedback for improving the generator. During the training process, the critic's objective is to maximize the Wasserstein distance, making the scores for real and fake data as distinct as possible.</p>","metadata":{}},{"cell_type":"code","source":"class Critic(nn.Module):\n    def __init__(self, im_chan=3, hidden_dim=128):\n        super(Critic, self).__init__()\n        self.crit = nn.Sequential(\n            # Image (3x128x128)\n            self.make_crit_block(im_chan, hidden_dim),\n            # State = 128 x 64 x 64\n            self.make_crit_block(hidden_dim, hidden_dim * 2),\n            # State = 256 x 32 x 32\n            self.make_crit_block(hidden_dim * 2, hidden_dim * 4),\n            # State = 512 x 16 x 16\n            self.make_crit_block(hidden_dim * 4, hidden_dim * 8),\n            # State = 1024 x 8 x 8\n            self.make_crit_block(hidden_dim * 8, hidden_dim * 16),\n            # State = 2048 x 4 x 4\n            self.make_crit_block(hidden_dim * 16, 1, stride = 1, padding = 0, final_layer=True),\n            # Final state = 1 x 1 x 1\n        )\n\n    def make_crit_block(self, input_channels, output_channels, kernel_size=4, stride=2, padding = 1, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding = padding),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding = padding),\n            )\n\n    def forward(self, image):\n        crit_pred = self.crit(image)\n        return crit_pred.view(len(crit_pred), -1)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:24:21.383091Z","iopub.execute_input":"2024-07-20T18:24:21.383720Z","iopub.status.idle":"2024-07-20T18:24:21.393226Z","shell.execute_reply.started":"2024-07-20T18:24:21.383688Z","shell.execute_reply":"2024-07-20T18:24:21.392273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Data Preprocessing </h2>\n<p style=\"font-size: 15px;\"> The images are loaded and resized into a shape of 128 x 128 x 3. The transformations applied include a center crop, a random horizontal flip with a probability of 0.5, color jitter, and random rotation with a probability of 0.2. Normalization was also done by subtracting the mean and dividing by the standard deviation.  </p>","metadata":{}},{"cell_type":"code","source":"# Data preprocessing\nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, transform=None):\n        self.image_paths = image_paths\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n\n        # Read and process image\n        img = Image.open(image_path).convert(\"RGB\")\n        if self.transform:\n            img = self.transform(img)\n        # Ensures channels last format\n#         img = img.permute(1, 2, 0)\n\n\n        return img\n\n# Define transformations\nrandom_transforms = [transforms.ColorJitter(), transforms.RandomRotation(degrees=20)]\ntransform = transforms.Compose([transforms.Resize(128),\n                                transforms.CenterCrop(128),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomApply(random_transforms, p=0.2),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\n# Create dataset and dataloader\ncustom_dataset = CustomDataset(images, transform=transform)\ndataloader = DataLoader(custom_dataset, batch_size=64, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:28:05.812774Z","iopub.execute_input":"2024-07-20T18:28:05.813534Z","iopub.status.idle":"2024-07-20T18:28:05.822703Z","shell.execute_reply.started":"2024-07-20T18:28:05.813501Z","shell.execute_reply":"2024-07-20T18:28:05.821667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Sample images </h2>","metadata":{}},{"cell_type":"code","source":"item = next(iter(dataloader))\nprint(item.shape)\nshow_tensor_images(item)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:28:28.793793Z","iopub.execute_input":"2024-07-20T18:28:28.794653Z","iopub.status.idle":"2024-07-20T18:28:29.774971Z","shell.execute_reply.started":"2024-07-20T18:28:28.794617Z","shell.execute_reply":"2024-07-20T18:28:29.774095Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Setting hyperparameters </h2>","metadata":{}},{"cell_type":"code","source":"n_epochs = 100\nz_dim = 100\ndisplay_step = 10\nbatch_size = 64\nlr = 0.0002\nbeta_1 = 0.5\nbeta_2 = 0.999\nbetas = (beta_1, beta_2)\nc_lambda = 10\ncrit_repeats = 5\nif torch.cuda.is_available():\n    device = 'cuda'\nelse:\n    device = 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:28:59.998398Z","iopub.execute_input":"2024-07-20T18:28:59.998770Z","iopub.status.idle":"2024-07-20T18:29:00.006590Z","shell.execute_reply.started":"2024-07-20T18:28:59.998742Z","shell.execute_reply":"2024-07-20T18:29:00.005621Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Initialization </h2>\n<p style=\"font-size: 15px;\"> The generator and the critic are initialized. Weights of the model are initialized using a normal distribution </p>","metadata":{}},{"cell_type":"code","source":"gen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas = betas)\ncritic = Critic().to(device)\ncrit_opt = torch.optim.Adam(critic.parameters(), lr = lr, betas = betas)\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\ngen = gen.apply(weights_init)\ncritic = critic.apply(weights_init)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:30:24.477091Z","iopub.execute_input":"2024-07-20T18:30:24.477985Z","iopub.status.idle":"2024-07-20T18:30:25.030161Z","shell.execute_reply.started":"2024-07-20T18:30:24.477947Z","shell.execute_reply":"2024-07-20T18:30:25.029364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Summary of the generator and the critic </h2>","metadata":{}},{"cell_type":"code","source":"summary(gen, input_size = (batch_size, z_dim))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:30:50.608654Z","iopub.execute_input":"2024-07-20T18:30:50.609622Z","iopub.status.idle":"2024-07-20T18:30:51.129691Z","shell.execute_reply.started":"2024-07-20T18:30:50.609585Z","shell.execute_reply":"2024-07-20T18:30:51.128799Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(critic, input_size = (batch_size, 3, 128, 128))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:31:05.137977Z","iopub.execute_input":"2024-07-20T18:31:05.138877Z","iopub.status.idle":"2024-07-20T18:31:05.253776Z","shell.execute_reply.started":"2024-07-20T18:31:05.138839Z","shell.execute_reply":"2024-07-20T18:31:05.252878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Gradient penalty </h2>\n<p style=\"font-size: 15px;\"> A crucial aspect of the critic's training is ensuring that it remains 1-Lipschitz continuous, which is typically enforced through a technique called gradient penalty. A function is Lipschitz continuous if there exists a constant \n𝐾 such that for any two points a and b, the absolute difference between the function's values at these points is bounded by 𝐾 times the distance between a and b. In mathematical terms, for a function 𝑓, </p>\n\n<p style=\"font-size: 15px; text-align: center;\"> ∣𝑓(b) − 𝑓(a)∣ ≤ 𝐾 ∥b−a∥.\n\n<p style=\"font-size: 15px;\"> In the context of WGAN, we require the critic function \n𝐷 to be 1-Lipschitz continuous, meaning 𝐾=1. This constraint is necessary for the theoretical properties of the Wasserstein distance to hold, ensuring stable and meaningful gradients. Mathematically, the above equation is equivalent to ensuring that the norm of the function's gradient must not be greater than 1. </p>\n\n<p style=\"font-size: 15px;\"> In order to enforce this constraint, the gradient penalty method was used. Introduced by I. Gulrajani (https://arxiv.org/abs/1704.00028), this method involves adding a regularization term to the loss function which penalizes the critic when the gradient norm is higher than one (to satisfy the 1-Lipschitz constraint). Since measuring the gradient of the critic at every possible point of the feature space is impractical, the authors come up with a clever way to assess when the gradient is higher than one. Instead of sampling all the points, a new image interpolated between the real and fake images is generated and is passed through the discriminator. The gradient obtained from this output is penalized if higher than one. Through this method, the 1-Lipschitz constraint is not enforced at all points but is rather encouraged. This soft constraint has been observed to work well in practice.","metadata":{}},{"cell_type":"code","source":"# Getting gradient for gradient penalty\ndef get_gradient(critic, real, fake, epsilon):\n    interpolated_img = real * epsilon + fake * (1 - epsilon)\n    y_hat = critic(interpolated_img)\n    gradient = torch.autograd.grad(\n        inputs = interpolated_img,\n        outputs = y_hat,\n        grad_outputs = torch.ones_like(y_hat),\n        create_graph = True,\n        retain_graph = True,\n    )[0]\n    return gradient\n","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:49:33.253047Z","iopub.execute_input":"2024-07-20T18:49:33.253436Z","iopub.status.idle":"2024-07-20T18:49:33.259165Z","shell.execute_reply.started":"2024-07-20T18:49:33.253406Z","shell.execute_reply":"2024-07-20T18:49:33.258223Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Computing gradient penalty\ndef gradient_penalty(gradient):\n    gradient = gradient.view(len(gradient), -1)\n    norm = gradient.norm(2, dim = 1)\n    return torch.mean((norm - 1)**2)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:49:33.598349Z","iopub.execute_input":"2024-07-20T18:49:33.598727Z","iopub.status.idle":"2024-07-20T18:49:33.603704Z","shell.execute_reply.started":"2024-07-20T18:49:33.598696Z","shell.execute_reply":"2024-07-20T18:49:33.602834Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Critic Loss </h2>\n<p style=\"font-size: 15px;\"> The critic maximizes the difference between the original distribution and the generated distribution. Or in mathematical terms, the critic maximizes </p>\n\\begin{equation}\n\\mathbb{E}_{\\mathbf{x} \\sim p_\\text{data}} [D(\\mathbf{x})] - \\mathbb{E}_{\\mathbf{z} \\sim p_\\mathbf{z}} [D(G(\\mathbf{z}))]\n\\end{equation}\n<br>\n<p style=\"font-size: 15px;\"> Maximizing an equation is the same as minimizing the negative. So, the critic tries to minimize:\n\n\\begin{equation}\nL_D = \\mathbb{E}_{\\mathbf{z} \\sim p_\\mathbf{z}} [D(G(\\mathbf{z}))] - \\mathbb{E}_{\\mathbf{x} \\sim p_\\text{data}} [D(\\mathbf{x})] + \\lambda \\mathbb{E}_{\\hat{\\mathbf{x}} \\sim p_{\\hat{\\mathbf{x}}}} \\left[ (\\|\\nabla_{\\hat{\\mathbf{x}}} D(\\hat{\\mathbf{x}})\\|_2 - 1)^2 \\right]\n\\end{equation}\n<br>\n<p style=\"font-size: 15px;\">The first term is the regular Earth Mover's loss function, the second term is the Gradient Penalty regularization term </p>\n","metadata":{}},{"cell_type":"code","source":"def get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda):\n    return torch.mean(crit_fake_pred) - torch.mean(crit_real_pred) + c_lambda * gp","metadata":{"execution":{"iopub.status.busy":"2024-07-25T00:19:52.458990Z","iopub.execute_input":"2024-07-25T00:19:52.459748Z","iopub.status.idle":"2024-07-25T00:19:52.464032Z","shell.execute_reply.started":"2024-07-25T00:19:52.459710Z","shell.execute_reply":"2024-07-25T00:19:52.463185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Generator Loss </h2>\n\\begin{equation}\nL_G = -\\mathbb{E}_{\\mathbf{z} \\sim p_\\mathbf{z}} [D(G(\\mathbf{z}))]\n\\end{equation}\n<br>","metadata":{}},{"cell_type":"code","source":"def get_gen_loss(crit_fake_pred):\n    return -torch.mean(crit_fake_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:54:06.835579Z","iopub.execute_input":"2024-07-20T18:54:06.835977Z","iopub.status.idle":"2024-07-20T18:54:06.841247Z","shell.execute_reply.started":"2024-07-20T18:54:06.835949Z","shell.execute_reply":"2024-07-20T18:54:06.839684Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def visualize_code(generator_losses, critic_losses, curr_step, real, fake, display_step = 10):\n    gen_mean = sum(generator_losses[-display_step:]) / display_step\n    crit_mean = sum(critic_losses[-display_step:]) / display_step\n    print(f\"Step {curr_step}: Generator loss: {gen_mean}, critic loss: {crit_mean}\")\n    show_tensor_images(fake)\n    show_tensor_images(real)\n    step_bins = 20\n    num_examples = (len(generator_losses) // step_bins) * step_bins\n    plt.plot(\n        range(num_examples // step_bins), \n        torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n        label=\"Generator Loss\"\n    )\n    plt.plot(\n        range(num_examples // step_bins), \n        torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n        label=\"Critic Loss\"\n    )\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:54:19.974283Z","iopub.execute_input":"2024-07-20T18:54:19.975027Z","iopub.status.idle":"2024-07-20T18:54:19.982232Z","shell.execute_reply.started":"2024-07-20T18:54:19.974994Z","shell.execute_reply":"2024-07-20T18:54:19.981370Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Training loop </h2>\n\n<p style=\"font-size: 15px;\"> The model is trained for 500 epochs. The critic is updated 5 times for every 1 update of the generator to provide meaningful information to the generator. </p>","metadata":{}},{"cell_type":"code","source":"curr_step = 0\ngenerator_losses = []\ncritic_losses = []\n\nfor epoch in range(0): # Change this to n_epochs to start training\n    for real in tqdm(dataloader):\n        real = real.to(device)\n        # Update critic\n        num_of_samples = real.shape[0]\n        # Critic is updated more times than the generator\n        sum_crit_loss = 0\n        for _ in range(crit_repeats): \n            crit_opt.zero_grad()\n            noise = get_noise(num_of_samples, z_dim, device)\n            fake = gen(noise)\n            crit_fake_pred = critic(fake.detach())\n            crit_real_pred = critic(real)\n            \n            epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n\n            gradient = get_gradient(critic, real, fake.detach(), epsilon)\n            gp = gradient_penalty(gradient)\n            crit_loss = get_crit_loss(crit_fake_pred, crit_real_pred, gp, c_lambda)\n            sum_crit_loss += crit_loss.item()\n            \n            crit_loss.backward(retain_graph = True)\n            crit_opt.step()\n            \n        mean_crit_loss = sum_crit_loss/crit_repeats\n        critic_losses.append(mean_crit_loss)\n        \n        # Update generator\n        gen_opt.zero_grad()\n        noise = get_noise(num_of_samples, z_dim, device)\n        fake = gen(noise)\n        crit_fake_pred = critic(fake)\n        gen_loss = get_gen_loss(crit_fake_pred)\n        gen_loss.backward()\n        gen_opt.step()\n        generator_losses.append(gen_loss.item())\n        \n        # Visualization step\n        if curr_step % display_step == 0 and curr_step > 0:\n            visualize_code(generator_losses, critic_losses, curr_step, real, fake, display_step)\n        curr_step += 1","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:54:51.382353Z","iopub.execute_input":"2024-07-20T18:54:51.382755Z","iopub.status.idle":"2024-07-20T18:54:51.395677Z","shell.execute_reply.started":"2024-07-20T18:54:51.382724Z","shell.execute_reply":"2024-07-20T18:54:51.394611Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Saving model\n# torch.save(gen.state_dict(), 'gen128_500.pth')\n# torch.save(critic.state_dict(), 'critic128_500.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T17:07:38.950540Z","iopub.execute_input":"2024-01-11T17:07:38.951354Z","iopub.status.idle":"2024-01-11T17:07:39.401572Z","shell.execute_reply.started":"2024-01-11T17:07:38.951319Z","shell.execute_reply":"2024-01-11T17:07:39.400768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Sampling from the model </h2>","metadata":{}},{"cell_type":"code","source":"gen = Generator(z_dim).to(device)\ngen_opt = torch.optim.Adam(gen.parameters(), lr = lr, betas = betas)\ncritic = Critic().to(device)\ncrit_opt = torch.optim.Adam(critic.parameters(), lr = lr, betas = betas)\n\n# Load the saved state dictionaries\n# This doesnt work when trying to save the notebook\n# gen.load_state_dict(torch.load('gen128_500.pth'))\n# critic.load_state_dict(torch.load('critic128_500.pth'))\n\n# Displaying generator outcome\n# noise = get_noise(64, z_dim, device)\n# fake = gen(noise)\n# show_tensor_images(fake)","metadata":{"execution":{"iopub.status.busy":"2024-07-20T18:55:11.965482Z","iopub.execute_input":"2024-07-20T18:55:11.965861Z","iopub.status.idle":"2024-07-20T18:55:13.129962Z","shell.execute_reply.started":"2024-07-20T18:55:11.965832Z","shell.execute_reply":"2024-07-20T18:55:13.129050Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<h2> Displaying output from generator","metadata":{}},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Display the saved image\ndisplay(Image(filename='/kaggle/input/gen-image/image-gen.png', width=400, height=300))","metadata":{"execution":{"iopub.status.busy":"2024-07-20T20:00:32.075316Z","iopub.execute_input":"2024-07-20T20:00:32.076212Z","iopub.status.idle":"2024-07-20T20:00:32.106889Z","shell.execute_reply.started":"2024-07-20T20:00:32.076174Z","shell.execute_reply":"2024-07-20T20:00:32.105846Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from torchvision.utils import save_image\n# import os\n# directory = \"/kaggle/working/output_imgs/\"\n# if not os.path.exists(directory):\n#     os.mkdir(directory)\n# def save_imgs(image_tensor, offset):\n#     offset = 600 * offset\n#     image_tensor = (image_tensor + 1) / 2\n#     image_unflat = image_tensor.detach().cpu()\n#     for i in range(image_unflat.shape[0]):\n#         img = image_unflat[i]\n#         save_image(img, directory + f\"img{i + offset}_test.png\")","metadata":{"execution":{"iopub.status.busy":"2024-01-11T19:08:56.566661Z","iopub.execute_input":"2024-01-11T19:08:56.567602Z","iopub.status.idle":"2024-01-11T19:08:56.573926Z","shell.execute_reply.started":"2024-01-11T19:08:56.567567Z","shell.execute_reply":"2024-01-11T19:08:56.572934Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Loading images to a working folder \n# for i in range(10):\n#     noise = get_noise(600, z_dim, device)\n#     fake_imgs = gen(noise)\n#     save_imgs(fake_imgs, i)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Converting output to zip\n# !zip -r \"/kaggle/working/output_imgs_final.zip\" \"/kaggle/working/output_imgs/\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}